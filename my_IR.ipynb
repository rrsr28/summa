{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "svwHReTjWn-Y",
        "d9dvTKvmZafY"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **SVD**"
      ],
      "metadata": {
        "id": "svwHReTjWn-Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLoquEfWV-ad"
      },
      "outputs": [],
      "source": [
        "# Full Scratch SVD\n",
        "# ----------------\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "# -----------------------------\n",
        "# Step 1: Preprocess documents\n",
        "# -----------------------------\n",
        "def preprocess(doc):\n",
        "    return doc.lower().split()\n",
        "\n",
        "# -----------------------------\n",
        "# Step 2: TF and TF-IDF functions\n",
        "# -----------------------------\n",
        "def compute_tf(documents):\n",
        "    \"\"\"Returns TF matrix\"\"\"\n",
        "    vocab = sorted(set(word for doc in documents for word in preprocess(doc)))\n",
        "    vocab_index = {word: i for i, word in enumerate(vocab)}\n",
        "    tf_matrix = np.zeros((len(vocab), len(documents)))\n",
        "\n",
        "    for j, doc in enumerate(documents):\n",
        "        words = preprocess(doc)\n",
        "        word_counts = Counter(words)\n",
        "        for word, count in word_counts.items():\n",
        "            tf_matrix[vocab_index[word], j] = count / len(words)\n",
        "\n",
        "    return tf_matrix, vocab\n",
        "\n",
        "def compute_tfidf(documents):\n",
        "    \"\"\"Returns TF-IDF matrix\"\"\"\n",
        "    vocab = sorted(set(word for doc in documents for word in preprocess(doc)))\n",
        "    vocab_index = {word: i for i, word in enumerate(vocab)}\n",
        "    N = len(documents)\n",
        "    tfidf_matrix = np.zeros((len(vocab), N))\n",
        "\n",
        "    # compute IDF\n",
        "    idf = {}\n",
        "    for word in vocab:\n",
        "        df = sum(1 for doc in documents if word in preprocess(doc))\n",
        "        idf[word] = math.log((N + 1) / (df + 1)) + 1  # smoothed IDF\n",
        "\n",
        "    # compute TF-IDF\n",
        "    for j, doc in enumerate(documents):\n",
        "        words = preprocess(doc)\n",
        "        word_counts = Counter(words)\n",
        "        for word, count in word_counts.items():\n",
        "            tf = count / len(words)\n",
        "            tfidf_matrix[vocab_index[word], j] = tf * idf[word]\n",
        "\n",
        "    return tfidf_matrix, vocab\n",
        "\n",
        "# -----------------------------\n",
        "# Step 3: Custom SVD via Eigen\n",
        "# -----------------------------\n",
        "def svd_from_scratch(A, k=None):\n",
        "    # A is term-document matrix\n",
        "    # Compute covariance matrices\n",
        "    ATA = np.dot(A.T, A)   # document-document\n",
        "    eigvals, eigvecs = np.linalg.eigh(ATA)  # eigen decomposition\n",
        "\n",
        "    # Sort eigenvalues & vectors in descending order\n",
        "    idx = np.argsort(-eigvals)\n",
        "    eigvals = eigvals[idx]\n",
        "    eigvecs = eigvecs[:, idx]\n",
        "\n",
        "    # Singular values are sqrt of eigenvalues\n",
        "    singular_vals = np.sqrt(np.maximum(eigvals, 0))\n",
        "\n",
        "    # Compute V and Sigma\n",
        "    V = eigvecs\n",
        "    Sigma = np.diag(singular_vals)\n",
        "\n",
        "    # Compute U = A V Σ⁻¹\n",
        "    U = np.dot(A, np.dot(V, np.linalg.inv(Sigma)))\n",
        "\n",
        "    if k:  # Truncate\n",
        "        U = U[:, :k]\n",
        "        Sigma = Sigma[:k, :k]\n",
        "        V = V[:, :k]\n",
        "\n",
        "    return U, Sigma, V\n",
        "\n",
        "# -----------------------------\n",
        "# Step 4: Cosine similarity\n",
        "# -----------------------------\n",
        "def cosine_similarity(a, b):\n",
        "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b) + 1e-9)\n",
        "\n",
        "# -----------------------------\n",
        "# Step 5: Example usage\n",
        "# -----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    documents = [\n",
        "        \"Information retrieval is the process of obtaining information\",\n",
        "        \"Machine learning improves information retrieval\",\n",
        "        \"Deep learning advances machine intelligence\"\n",
        "    ]\n",
        "\n",
        "    # Choose representation: TF or TF-IDF\n",
        "    A, vocab = compute_tfidf(documents)  # or compute_tf(documents)\n",
        "    print(\"Original Matrix (TF-IDF):\")\n",
        "    print(A)\n",
        "\n",
        "    # Full SVD\n",
        "    # U, S, VT = np.linalg.svd(X, full_matrices=False)\n",
        "    U, S, V = svd_from_scratch(A, k=None)\n",
        "    print(\"\\nFull Reconstruction:\")\n",
        "    print(np.dot(U, np.dot(S, V.T)))\n",
        "\n",
        "    # Truncated SVD (k=2)\n",
        "    # svd = TruncatedSVD(n_components=2)\n",
        "    # X_reduced = svd.fit_transform(X)\n",
        "    U_k, S_k, V_k = svd_from_scratch(A, k=2)\n",
        "    print(\"\\nTruncated Reconstruction (k=2):\")\n",
        "    print(np.dot(U_k, np.dot(S_k, V_k.T)))\n",
        "\n",
        "    # Query\n",
        "    query = \"information learning\"\n",
        "    query_vec = np.zeros((len(vocab),))\n",
        "    words = preprocess(query)\n",
        "    for word in words:\n",
        "        if word in vocab:\n",
        "            query_vec[vocab.index(word)] += 1\n",
        "    # Project query into reduced space\n",
        "    q_proj = np.dot(query_vec, U_k)\n",
        "    docs_proj = np.dot(A.T, U_k)\n",
        "\n",
        "    print(\"\\nCosine Similarities (Query vs Docs in reduced space):\")\n",
        "    for i, doc_proj in enumerate(docs_proj):\n",
        "        sim = cosine_similarity(q_proj, doc_proj)\n",
        "        print(f\"Doc {i+1}: {sim:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MinHash**"
      ],
      "metadata": {
        "id": "d9dvTKvmZafY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MinHash using permutation\n",
        "# -------------------------\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Example documents\n",
        "docs = [\n",
        "    \"the cat sat on the mat\",\n",
        "    \"the dog sat on the mat\",\n",
        "    \"the cat chased the dog\"\n",
        "]\n",
        "\n",
        "# Step 1: Shingling (k=2 or 3)\n",
        "def get_shingles(doc, k=2):\n",
        "    words = doc.split()\n",
        "    return {\" \".join(words[i:i+k]) for i in range(len(words)-k+1)}\n",
        "\n",
        "k = 2\n",
        "shingle_set = set()\n",
        "doc_shingles = []\n",
        "\n",
        "for d in docs:\n",
        "    sh = get_shingles(d, k)\n",
        "    doc_shingles.append(sh)\n",
        "    shingle_set |= sh\n",
        "\n",
        "shingles = list(shingle_set)\n",
        "\n",
        "# Step 2: Build binary shingle–document matrix\n",
        "matrix = []\n",
        "for sh in shingles:\n",
        "    row = [1 if sh in doc_shingles[j] else 0 for j in range(len(docs))]\n",
        "    matrix.append(row)\n",
        "\n",
        "sd_matrix = np.array(matrix)\n",
        "\n",
        "print(\"Shingle–Document Matrix:\")\n",
        "print(sd_matrix)\n",
        "\n",
        "# Step 3: MinHash Implementation\n",
        "num_shingles, num_docs = sd_matrix.shape\n",
        "num_hashes = 5  # number of permutations\n",
        "signature = np.full((num_hashes, num_docs), np.inf)\n",
        "\n",
        "rows = list(range(num_shingles))\n",
        "permutations = [random.sample(rows, len(rows)) for _ in range(num_hashes)]\n",
        "\n",
        "for i, perm in enumerate(permutations):\n",
        "    for col in range(num_docs):\n",
        "        for row in perm:\n",
        "            if sd_matrix[row, col] == 1:\n",
        "                signature[i, col] = row\n",
        "                break\n",
        "\n",
        "print(\"\\nSignature Matrix:\")\n",
        "print(signature)\n",
        "\n",
        "# Step 4: Similarity from signatures\n",
        "def minhash_sim(col1, col2):\n",
        "    return np.mean(signature[:, col1] == signature[:, col2])\n",
        "\n",
        "print(\"\\nSimilarity between Doc1 & Doc2:\", minhash_sim(0, 1))\n",
        "print(\"Similarity between Doc1 & Doc3:\", minhash_sim(0, 2))\n",
        "print(\"Similarity between Doc2 & Doc3:\", minhash_sim(1, 2))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# MinHash using hash function\n",
        "# ---------------------------\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Example documents\n",
        "docs = [\n",
        "    \"the cat sat on the mat\",\n",
        "    \"the dog sat on the mat\",\n",
        "    \"the cat chased the dog\"\n",
        "]\n",
        "\n",
        "# Step 1: Shingling (k=2 or 3)\n",
        "def get_shingles(doc, k=2):\n",
        "    words = doc.split()\n",
        "    return {\" \".join(words[i:i+k]) for i in range(len(words)-k+1)}\n",
        "\n",
        "k = 2\n",
        "shingle_set = set()\n",
        "doc_shingles = []\n",
        "\n",
        "for d in docs:\n",
        "    sh = get_shingles(d, k)\n",
        "    doc_shingles.append(sh)\n",
        "    shingle_set |= sh\n",
        "\n",
        "shingles = list(shingle_set)\n",
        "num_shingles = len(shingles)\n",
        "\n",
        "# Step 2: Build binary shingle–document matrix\n",
        "matrix = []\n",
        "for sh in shingles:\n",
        "    row = [1 if sh in doc_shingles[j] else 0 for j in range(len(docs))]\n",
        "    matrix.append(row)\n",
        "\n",
        "sd_matrix = np.array(matrix)\n",
        "\n",
        "print(\"Shingle–Document Matrix:\")\n",
        "print(sd_matrix)\n",
        "\n",
        "# Step 3: MinHash using hash functions\n",
        "num_shingles, num_docs = sd_matrix.shape\n",
        "num_hashes = 5  # number of hash functions\n",
        "signature = np.full((num_hashes, num_docs), np.inf)\n",
        "\n",
        "# Create random hash functions of form: h(x) = (a*x + b) % p\n",
        "p = 2 * num_shingles  # prime > num_shingles\n",
        "hash_funcs = [(random.randint(1, p-1), random.randint(0, p-1)) for _ in range(num_hashes)]\n",
        "\n",
        "for i, (a, b) in enumerate(hash_funcs):\n",
        "    for row in range(num_shingles):\n",
        "        hash_val = (a * row + b) % p\n",
        "        for col in range(num_docs):\n",
        "            if sd_matrix[row, col] == 1:\n",
        "                if hash_val < signature[i, col]:\n",
        "                    signature[i, col] = hash_val\n",
        "\n",
        "print(\"\\nSignature Matrix:\")\n",
        "print(signature.astype(int))\n",
        "\n",
        "# Step 4: Similarity from signatures\n",
        "def minhash_sim(col1, col2):\n",
        "    return np.mean(signature[:, col1] == signature[:, col2])\n",
        "\n",
        "print(\"\\nSimilarity between Doc1 & Doc2:\", minhash_sim(0, 1))\n",
        "print(\"Similarity between Doc1 & Doc3:\", minhash_sim(0, 2))\n",
        "print(\"Similarity between Doc2 & Doc3:\", minhash_sim(1, 2))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Cosine similarity And Euclidean Case\n",
        "# ------------------------------------\n",
        "\n",
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "\n",
        "# Example documents\n",
        "docs = [\n",
        "    \"this is a cat\",\n",
        "    \"this is a dog\",\n",
        "    \"cats and dogs are animals\",\n",
        "    \"the dog chased the cat\"\n",
        "]\n",
        "\n",
        "# Step 1: Vocabulary\n",
        "words = list(set(\" \".join(docs).split()))\n",
        "\n",
        "# Step 2: Term Frequency (TF)\n",
        "tf = []\n",
        "for d in docs:\n",
        "    row = [d.split().count(w) for w in words]\n",
        "    tf.append(row)\n",
        "tf = np.array(tf)\n",
        "\n",
        "# Step 3: Inverse Document Frequency (IDF)\n",
        "N = len(docs)\n",
        "idf = np.log(N / (np.count_nonzero(tf, axis=0)))\n",
        "\n",
        "# Step 4: TF-IDF\n",
        "tfidf = tf * idf\n",
        "print(\"TF-IDF Matrix:\\n\", tfidf)\n",
        "\n",
        "# Step 5: Pairwise Cosine Similarity and Euclidean Distance\n",
        "num_docs = len(docs)\n",
        "\n",
        "print(\"\\nCosine Similarities:\")\n",
        "for i in range(num_docs):\n",
        "    for j in range(i+1, num_docs):\n",
        "        cosine_sim = np.dot(tfidf[i], tfidf[j]) / (norm(tfidf[i]) * norm(tfidf[j]))\n",
        "        print(f\"Doc{i+1} vs Doc{j+1}: {cosine_sim:.4f}\")\n",
        "\n",
        "print(\"\\nEuclidean Distances:\")\n",
        "for i in range(num_docs):\n",
        "    for j in range(i+1, num_docs):\n",
        "        euclidean_dist = norm(tfidf[i] - tfidf[j])\n",
        "        print(f\"Doc{i+1} vs Doc{j+1}: {euclidean_dist:.4f}\")"
      ],
      "metadata": {
        "id": "5qTV0OYGZcCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "\n",
        "# ------------------ MinHash Part ------------------\n",
        "\n",
        "def build_shingle_matrix(docs, k=2):\n",
        "    \"\"\"\n",
        "    Build binary shingle-document matrix using k-word shingles.\n",
        "    \"\"\"\n",
        "    shingles = set()\n",
        "    doc_shingles = []\n",
        "\n",
        "    for doc in docs:\n",
        "        words = doc.lower().split()\n",
        "        # Create k-word shingles\n",
        "        s = {\" \".join(words[i:i+k]) for i in range(len(words) - k + 1)}\n",
        "        shingles |= s\n",
        "        doc_shingles.append(s)\n",
        "\n",
        "    shingles = list(shingles)\n",
        "    shingle_index = {s: i for i, s in enumerate(shingles)}\n",
        "    # {'quick brown dog': 0, 'fast brown fox': 1, 'the fast brown': 2, 'quick brown fox': 3, 'the quick brown': 4}\n",
        "    # print(shingle_index)\n",
        "\n",
        "    # Binary matrix\n",
        "    M = np.zeros((len(shingles), len(docs)), dtype=int)\n",
        "    for j, s_set in enumerate(doc_shingles):\n",
        "        for s in s_set:\n",
        "            M[shingle_index[s], j] = 1\n",
        "\n",
        "    return M, shingles\n",
        "\n",
        "\n",
        "\n",
        "def minhash_signature(M, num_hashes=100):\n",
        "    \"\"\"\n",
        "    Generate signature matrix using MinHash.\n",
        "    \"\"\"\n",
        "    n_shingles, n_docs = M.shape\n",
        "    sig = np.full((num_hashes, n_docs), np.inf)\n",
        "\n",
        "    # Random hash functions: (a*x + b) % p\n",
        "    p = 2**31 - 1\n",
        "    hash_funcs = [(random.randint(1, p-1), random.randint(0, p-1)) for _ in range(num_hashes)]\n",
        "\n",
        "    for r in range(n_shingles):\n",
        "        row = M[r]\n",
        "        for h, (a, b) in enumerate(hash_funcs):\n",
        "            hash_val = (a*r + b) % p\n",
        "            for c in range(n_docs):\n",
        "                if row[c] == 1:\n",
        "                    sig[h, c] = min(sig[h, c], hash_val)\n",
        "    return sig\n",
        "\n",
        "def minhash_signature_permutation(M, num_permutations=100):\n",
        "    \"\"\"\n",
        "    Generate signature matrix using MinHash with random row permutations.\n",
        "    \"\"\"\n",
        "    n_shingles, n_docs = M.shape\n",
        "    sig = np.full((num_permutations, n_docs), np.inf)\n",
        "\n",
        "    for p in range(num_permutations):\n",
        "        # Create a random permutation of rows\n",
        "        perm = np.random.permutation(n_shingles)\n",
        "        for c in range(n_docs):\n",
        "            # Find the first row (in perm order) where doc has a 1\n",
        "            for idx in perm:\n",
        "                if M[idx, c] == 1:\n",
        "                    sig[p, c] = idx\n",
        "                    break\n",
        "    return sig\n",
        "\n",
        "def jaccard_from_signatures(sig, i, j):\n",
        "    \"\"\"\n",
        "    Approximate Jaccard similarity between documents i and j.\n",
        "    \"\"\"\n",
        "    return np.mean(sig[:, i] == sig[:, j])\n",
        "\n",
        "\n",
        "# ------------------ TF-IDF Part ------------------\n",
        "\n",
        "def tokenize(doc):\n",
        "    return doc.lower().split()\n",
        "\n",
        "def build_count_matrix(docs):\n",
        "    \"\"\"\n",
        "    Count matrix (term-document).\n",
        "    \"\"\"\n",
        "    vocab = {}\n",
        "    tokenized_docs = []\n",
        "    for doc in docs:\n",
        "        tokens = tokenize(doc)\n",
        "        tokenized_docs.append(tokens)\n",
        "        for t in tokens:\n",
        "            if t not in vocab:\n",
        "                vocab[t] = len(vocab)\n",
        "\n",
        "    V = len(vocab)\n",
        "    D = len(docs)\n",
        "    count_matrix = np.zeros((V, D), dtype=float)\n",
        "\n",
        "    for j, tokens in enumerate(tokenized_docs):\n",
        "        for t in tokens:\n",
        "            i = vocab[t]\n",
        "            count_matrix[i, j] += 1\n",
        "\n",
        "    return count_matrix, vocab\n",
        "\n",
        "def compute_tfidf(count_matrix):\n",
        "    \"\"\"\n",
        "    Compute TF-IDF matrix manually.\n",
        "    \"\"\"\n",
        "    V, D = count_matrix.shape\n",
        "    tfidf = np.zeros((V, D))\n",
        "\n",
        "    # Document frequencies\n",
        "    df = np.count_nonzero(count_matrix > 0, axis=1)\n",
        "\n",
        "    for j in range(D):\n",
        "        for i in range(V):\n",
        "            tf = count_matrix[i, j] / (np.sum(count_matrix[:, j]) + 1e-10)\n",
        "            idf = math.log((D + 1) / (df[i] + 1)) + 1\n",
        "            tfidf[i, j] = tf * idf\n",
        "\n",
        "    return tfidf\n",
        "\n",
        "def cosine_similarity(A):\n",
        "    \"\"\"\n",
        "    Compute cosine similarity matrix for document vectors.\n",
        "    \"\"\"\n",
        "    D = A.shape[1]\n",
        "    sim = np.zeros((D, D))\n",
        "    for i in range(D):\n",
        "        for j in range(D):\n",
        "            num = np.dot(A[:, i], A[:, j])\n",
        "            denom = (np.linalg.norm(A[:, i]) * np.linalg.norm(A[:, j]) + 1e-10)\n",
        "            sim[i, j] = num / denom\n",
        "    return sim\n",
        "\n",
        "def euclidean_distance(A):\n",
        "    \"\"\"\n",
        "    Compute Euclidean distance matrix for document vectors.\n",
        "    \"\"\"\n",
        "    D = A.shape[1]\n",
        "    dist = np.zeros((D, D))\n",
        "    for i in range(D):\n",
        "        for j in range(D):\n",
        "            dist[i, j] = np.linalg.norm(A[:, i] - A[:, j])\n",
        "    return dist\n",
        "\n",
        "\n",
        "# ------------------ Example Usage ------------------\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    docs = [\n",
        "        \"the quick brown fox\",\n",
        "        \"the quick brown dog\",\n",
        "        \"the fast brown fox\"\n",
        "    ]\n",
        "\n",
        "    print(\"\\n=== MinHash (Binary Shingle Matrix) ===\")\n",
        "    M, shingles = build_shingle_matrix(docs, k=3)\n",
        "    print(\"Binary Shingle-Doc Matrix:\\n\", M)\n",
        "\n",
        "    sig = minhash_signature(M, num_hashes=50)\n",
        "\n",
        "    # Hash function method\n",
        "    for i in range(len(docs)):\n",
        "        for j in range(i+1, len(docs)):\n",
        "            sim = jaccard_from_signatures(sig, i, j)\n",
        "            print(f\"Estimated Jaccard similarity (Doc {i}, Doc {j}): {sim:.3f}\")\n",
        "\n",
        "\n",
        "    # Permutation method\n",
        "    sig_perm = minhash_signature_permutation(M, num_permutations=50)\n",
        "    print(\"\\n--- Using Permutation Method ---\")\n",
        "    for i in range(len(docs)):\n",
        "        for j in range(i+1, len(docs)):\n",
        "            sim = jaccard_from_signatures(sig_perm, i, j)\n",
        "            print(f\"Estimated Jaccard similarity (Doc {i}, Doc {j}): {sim:.3f}\")\n",
        "\n",
        "\n",
        "\n",
        "    print(\"\\n=== TF-IDF (Cosine & Euclidean) ===\")\n",
        "    count_matrix, vocab = build_count_matrix(docs)\n",
        "    tfidf = compute_tfidf(count_matrix)\n",
        "\n",
        "    cos_sim = cosine_similarity(tfidf)\n",
        "    euc_dist = euclidean_distance(tfidf)\n",
        "\n",
        "    print(\"Cosine Similarity:\\n\", cos_sim)\n",
        "    print(\"Euclidean Distances:\\n\", euc_dist)"
      ],
      "metadata": {
        "id": "Wb_G1AFMaO4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PageRank**"
      ],
      "metadata": {
        "id": "RSsPWTnaafRl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def pagerank(adj_matrix, alpha=0.15, tol=1e-6, max_iter=100):\n",
        "    \"\"\"\n",
        "    PageRank using power iteration:\n",
        "    PR(k+1) = PR(k) * P\n",
        "    \"\"\"\n",
        "    n = adj_matrix.shape[0]\n",
        "\n",
        "    # Step 1: Build Hyperlink matrix H\n",
        "    H = adj_matrix.astype(float)\n",
        "    row_sums = H.sum(axis=1)\n",
        "\n",
        "    for i in range(n):\n",
        "        if row_sums[i] != 0:\n",
        "            H[i] /= row_sums[i]\n",
        "        else:\n",
        "            # Dangling node → distribute uniformly\n",
        "            H[i] = np.ones(n) / n\n",
        "\n",
        "    # Step 2: Build transition matrix P\n",
        "    P = (1 - alpha) * H + alpha * np.ones((n, n)) / n\n",
        "\n",
        "    # Step 3: Initialize PageRank vector\n",
        "    PR = np.ones(n) / n\n",
        "\n",
        "    # Step 4: Power iteration\n",
        "    for _ in range(max_iter):\n",
        "        new_PR = PR @ P\n",
        "        if np.linalg.norm(new_PR - PR, 1) < tol:\n",
        "            PR = new_PR\n",
        "            break\n",
        "        PR = new_PR\n",
        "        # print(PR)\n",
        "    return PR / PR.sum()  # Normalize\n",
        "\n",
        "def pagerank_eigen(adj_matrix, alpha=0.15):\n",
        "    \"\"\"\n",
        "    PageRank using eigenvector method.\n",
        "    \"\"\"\n",
        "    n = adj_matrix.shape[0]\n",
        "\n",
        "    # Step 1: Build Hyperlink matrix H\n",
        "    H = adj_matrix.astype(float)\n",
        "    row_sums = H.sum(axis=1)\n",
        "    for i in range(n):\n",
        "        if row_sums[i] != 0:\n",
        "            H[i] /= row_sums[i]\n",
        "        else:\n",
        "            H[i] = np.ones(n) / n\n",
        "\n",
        "    # Step 2: Build transition matrix P\n",
        "    P = (1 - alpha) * H + alpha * np.ones((n, n)) / n\n",
        "\n",
        "    # Step 3: Solve eigenvector problem for P^T\n",
        "    vals, vecs = np.linalg.eig(P.T)\n",
        "\n",
        "    # Find index of eigenvalue closest to 1\n",
        "    idx = np.argmin(np.abs(vals - 1))\n",
        "\n",
        "    # Get corresponding eigenvector\n",
        "    principal = np.real(vecs[:, idx])\n",
        "\n",
        "    # Normalize to make it a probability distribution\n",
        "    pr = principal / principal.sum()\n",
        "    return pr\n",
        "\n",
        "def plot_graph(adj_matrix):\n",
        "    # Create directed graph\n",
        "    G = nx.DiGraph(adj)\n",
        "\n",
        "    # Compute PageRank using NetworkX (built-in)\n",
        "    pr = nx.pagerank(G, alpha=0.85)\n",
        "    print(pr)\n",
        "\n",
        "    # Node sizes proportional to PageRank\n",
        "    node_sizes = [5000 * pr[node] for node in G.nodes()]\n",
        "\n",
        "    # Draw graph\n",
        "    pos = nx.spring_layout(G, seed=42)\n",
        "    nx.draw(G, pos, with_labels=True, node_size=node_sizes,\n",
        "            node_color=\"skyblue\", arrows=True, font_weight=\"bold\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    adj = np.array([\n",
        "        [0, 1, 1, 0],\n",
        "        [0, 0, 0, 1],\n",
        "        [1, 0, 0, 1],\n",
        "        [0, 0, 1, 0]\n",
        "    ])\n",
        "\n",
        "    pr = pagerank(adj, alpha=0.15)\n",
        "    print(\"PageRank:\", pr)\n",
        "\n",
        "    # Ranking documents\n",
        "    ranking = np.argsort(-pr)  # sort in descending order\n",
        "    print(\"Ranking of documents (best to worst):\", ranking)\n",
        "\n",
        "    plot_graph(adj)"
      ],
      "metadata": {
        "id": "v16oDZm2ai8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Page Rank using Power iteration\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def pagerank_eigen(adj_matrix, alpha=0.85):\n",
        "    n = adj_matrix.shape[0]\n",
        "\n",
        "    # Step 1: Construct hyperlink matrix H\n",
        "    H = np.zeros((n, n))\n",
        "    for i in range(n):\n",
        "        out_links = np.sum(adj_matrix[i])\n",
        "        if out_links > 0:\n",
        "            H[i] = adj_matrix[i] / out_links\n",
        "\n",
        "    # Step 2: Fix dangling nodes\n",
        "    for i in range(n):\n",
        "        if np.sum(H[i]) == 0:\n",
        "            H[i] = np.ones(n) / n\n",
        "\n",
        "    # Step 3: Google matrix\n",
        "    G = alpha * H + (1 - alpha) * (np.ones((n, n)) / n)\n",
        "\n",
        "    # Step 4: Eigen decomposition\n",
        "    eigvals, eigvecs = np.linalg.eig(G.T)\n",
        "\n",
        "    # Find the index of eigenvalue closest to 1\n",
        "    idx = np.argmin(np.abs(eigvals - 1))\n",
        "\n",
        "    # Corresponding eigenvector\n",
        "    principal_eigvec = np.real(eigvecs[:, idx])\n",
        "\n",
        "    # Normalize to sum to 1\n",
        "    pagerank = principal_eigvec / np.sum(principal_eigvec)\n",
        "\n",
        "    return pagerank\n",
        "\n",
        "\n",
        "def pagerank(adj_matrix, alpha=0.85, tol=1e-6, max_iter=100):\n",
        "\n",
        "    n = adj_matrix.shape[0]\n",
        "\n",
        "    # Step 1: Construct hyperlink matrix H\n",
        "    H = np.zeros((n, n))\n",
        "    for i in range(n):\n",
        "        out_links = np.sum(adj_matrix[i])\n",
        "        if out_links > 0:\n",
        "            H[i] = adj_matrix[i] / out_links\n",
        "\n",
        "    # Step 2: Fix dangling nodes\n",
        "    for i in range(n):\n",
        "        if np.sum(H[i]) == 0:\n",
        "            H[i] = np.ones(n) / n\n",
        "\n",
        "    # Step 3: Google matrix\n",
        "    G = alpha * H + (1 - alpha) * (np.ones((n, n)) / n)\n",
        "\n",
        "    # Step 4: Power iteration\n",
        "    rank = np.ones(n) / n\n",
        "    for _ in range(max_iter):\n",
        "        new_rank = rank @ G\n",
        "        if np.linalg.norm(new_rank - rank, 1) < tol:\n",
        "            break\n",
        "        rank = new_rank\n",
        "\n",
        "    return rank\n",
        "\n",
        "\n",
        "# ====== Generate Graph ======\n",
        "# Example: Directed graph\n",
        "G = nx.DiGraph()\n",
        "\n",
        "# Add edges (you can modify as needed)\n",
        "edges = [\n",
        "    (1, 2), (2, 5), (3, 1), (3, 2), (3, 4), (3, 5),\n",
        "    (4, 5), (5, 4)\n",
        "]\n",
        "G.add_edges_from(edges)\n",
        "\n",
        "# Draw graph\n",
        "plt.figure(figsize=(6, 6))\n",
        "pos = nx.spring_layout(G, seed=42)\n",
        "nx.draw(G, pos, with_labels=True, node_size=1000, node_color=\"skyblue\", font_size=12, arrows=True)\n",
        "plt.title(\"Directed Graph\")\n",
        "plt.show()\n",
        "\n",
        "# ====== Create adjacency matrix ======\n",
        "adj_matrix = nx.to_numpy_array(G, nodelist=sorted(G.nodes()), dtype=int)\n",
        "print(\"Adjacency Matrix:\\n\", adj_matrix)\n",
        "\n",
        "# ====== Run PageRank ======\n",
        "pagerank_scores = pagerank(adj_matrix)\n",
        "print(\"PageRank Scores:\", pagerank_scores)\n",
        "print(\"Sum of scores (should be 1):\", np.sum(pagerank_scores))"
      ],
      "metadata": {
        "id": "rdH0l-V-cegD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}